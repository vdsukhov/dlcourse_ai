{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```python\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "$$\\frac{f(x + h) - f(x - h)}{2 \\cdot h}$$\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/dlcourse_ai/assignments/assignment1/Linear classifier.ipynb Ячейка 9\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bvdsukhov-dlcourse-ai-57xvpjrp3p45q/workspaces/dlcourse_ai/assignments/assignment1/Linear%20classifier.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Make sure it works for big numbers too!\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bvdsukhov-dlcourse-ai-57xvpjrp3p45q/workspaces/dlcourse_ai/assignments/assignment1/Linear%20classifier.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m probs \u001b[39m=\u001b[39m linear_classifer\u001b[39m.\u001b[39msoftmax(np\u001b[39m.\u001b[39marray([\u001b[39m1000\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m]))\n\u001b[0;32m----> <a href='vscode-notebook-cell://codespaces%2Bvdsukhov-dlcourse-ai-57xvpjrp3p45q/workspaces/dlcourse_ai/assignments/assignment1/Linear%20classifier.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39massert\u001b[39;00m np\u001b[39m.\u001b[39misclose(probs[\u001b[39m0\u001b[39m], \u001b[39m1.0\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/dlcourse_ai/assignments/assignment1/Linear classifier.ipynb Ячейка 11\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bvdsukhov-dlcourse-ai-57xvpjrp3p45q/workspaces/dlcourse_ai/assignments/assignment1/Linear%20classifier.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m probs \u001b[39m=\u001b[39m linear_classifer\u001b[39m.\u001b[39msoftmax(np\u001b[39m.\u001b[39marray([\u001b[39m-\u001b[39m\u001b[39m5\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m5\u001b[39m]))\n\u001b[0;32m----> <a href='vscode-notebook-cell://codespaces%2Bvdsukhov-dlcourse-ai-57xvpjrp3p45q/workspaces/dlcourse_ai/assignments/assignment1/Linear%20classifier.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m linear_classifer\u001b[39m.\u001b[39;49mcross_entropy_loss(probs, \u001b[39m1\u001b[39;49m)\n",
      "File \u001b[0;32m/workspaces/dlcourse_ai/assignments/assignment1/linear_classifer.py:46\u001b[0m, in \u001b[0;36mcross_entropy_loss\u001b[0;34m(probs, target_index)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[39mComputes cross-entropy loss\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39m  loss: single value\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[39m# TODO implement cross-entropy\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[39m# Your final implementation shouldn't have any loops\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39m# raise Exception(\"Not implemented!\")\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[39m# shift = np.arange(0, target_index.shape[0] * probs.shape[1], probs.shape[1])\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[39m# tind = target_index.ravel() + shift\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[39m# return np.sum(-np.log(probs.ravel()[tind]))\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39msum(\u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39mlog(np\u001b[39m.\u001b[39mtake_along_axis(probs, target_index\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "linear_classifer.cross_entropy_loss(probs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/dlcourse_ai/assignments/assignment1/Linear classifier.ipynb Ячейка 13\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bvdsukhov-dlcourse-ai-57xvpjrp3p45q/workspaces/dlcourse_ai/assignments/assignment1/Linear%20classifier.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# TODO Implement combined function or softmax and cross entropy and produces gradient\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://codespaces%2Bvdsukhov-dlcourse-ai-57xvpjrp3p45q/workspaces/dlcourse_ai/assignments/assignment1/Linear%20classifier.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m loss, grad \u001b[39m=\u001b[39m linear_classifer\u001b[39m.\u001b[39;49msoftmax_with_cross_entropy(np\u001b[39m.\u001b[39;49marray([\u001b[39m1\u001b[39;49m, \u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m]), \u001b[39m1\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bvdsukhov-dlcourse-ai-57xvpjrp3p45q/workspaces/dlcourse_ai/assignments/assignment1/Linear%20classifier.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m check_gradient(\u001b[39mlambda\u001b[39;00m x: linear_classifer\u001b[39m.\u001b[39msoftmax_with_cross_entropy(x, \u001b[39m1\u001b[39m), np\u001b[39m.\u001b[39marray([\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m], np\u001b[39m.\u001b[39mfloat))\n",
      "File \u001b[0;32m/workspaces/dlcourse_ai/assignments/assignment1/linear_classifer.py:68\u001b[0m, in \u001b[0;36msoftmax_with_cross_entropy\u001b[0;34m(predictions, target_index)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39m# TODO implement softmax with cross-entropy\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[39m# Your final implementation shouldn't have any loops\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[39m# raise Exception(\"Not implemented!\")\u001b[39;00m\n\u001b[1;32m     67\u001b[0m s_vals \u001b[39m=\u001b[39m softmax(predictions)\n\u001b[0;32m---> 68\u001b[0m loss \u001b[39m=\u001b[39m cross_entropy_loss(s_vals, target_index)\n\u001b[1;32m     71\u001b[0m dprediction \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(predictions\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     72\u001b[0m np\u001b[39m.\u001b[39mput_along_axis(dprediction, target_index\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/workspaces/dlcourse_ai/assignments/assignment1/linear_classifer.py:46\u001b[0m, in \u001b[0;36mcross_entropy_loss\u001b[0;34m(probs, target_index)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[39mComputes cross-entropy loss\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39m  loss: single value\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[39m# TODO implement cross-entropy\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[39m# Your final implementation shouldn't have any loops\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39m# raise Exception(\"Not implemented!\")\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[39m# shift = np.arange(0, target_index.shape[0] * probs.shape[1], probs.shape[1])\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[39m# tind = target_index.ravel() + shift\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[39m# return np.sum(-np.log(probs.ravel()[tind]))\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39msum(\u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39mlog(np\u001b[39m.\u001b[39mtake_along_axis(probs, target_index\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(int)\n",
    "# print(predictions)\n",
    "# print(linear_classifer.softmax(predictions))\n",
    "# print(target_index)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# # Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "# print(probs)\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(float)\n",
    "target_index = np.ones(batch_size, dtype=int)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 20695.932443\n",
      "Epoch 1, loss: 20622.866864\n",
      "Epoch 2, loss: 20561.089563\n",
      "Epoch 3, loss: 20506.248325\n",
      "Epoch 4, loss: 20457.605612\n",
      "Epoch 5, loss: 20415.553486\n",
      "Epoch 6, loss: 20376.095990\n",
      "Epoch 7, loss: 20340.946337\n",
      "Epoch 8, loss: 20311.436765\n",
      "Epoch 9, loss: 20283.206647\n",
      "Epoch 10, loss: 20258.695498\n",
      "Epoch 11, loss: 20236.121670\n",
      "Epoch 12, loss: 20215.698763\n",
      "Epoch 13, loss: 20196.594447\n",
      "Epoch 14, loss: 20178.709886\n",
      "Epoch 15, loss: 20165.948865\n",
      "Epoch 16, loss: 20150.726911\n",
      "Epoch 17, loss: 20138.382445\n",
      "Epoch 18, loss: 20125.898417\n",
      "Epoch 19, loss: 20115.636141\n",
      "Epoch 20, loss: 20108.093668\n",
      "Epoch 21, loss: 20102.031606\n",
      "Epoch 22, loss: 20091.770361\n",
      "Epoch 23, loss: 20084.812300\n",
      "Epoch 24, loss: 20079.863748\n",
      "Epoch 25, loss: 20071.462837\n",
      "Epoch 26, loss: 20065.381866\n",
      "Epoch 27, loss: 20061.157853\n",
      "Epoch 28, loss: 20055.368309\n",
      "Epoch 29, loss: 20053.102801\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=30, learning_rate=1e-2, batch_size=300, reg=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f3841f426a0>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.124\n",
      "Epoch 0, loss: 20705.051710\n",
      "Epoch 1, loss: 20696.373612\n",
      "Epoch 2, loss: 20688.184570\n",
      "Epoch 3, loss: 20680.100741\n",
      "Epoch 4, loss: 20672.403032\n",
      "Epoch 5, loss: 20664.847375\n",
      "Epoch 6, loss: 20657.329346\n",
      "Epoch 7, loss: 20650.052922\n",
      "Epoch 8, loss: 20642.725762\n",
      "Epoch 9, loss: 20635.519868\n",
      "Epoch 10, loss: 20628.661281\n",
      "Epoch 11, loss: 20621.725470\n",
      "Epoch 12, loss: 20614.894972\n",
      "Epoch 13, loss: 20608.367270\n",
      "Epoch 14, loss: 20601.595595\n",
      "Epoch 15, loss: 20595.125287\n",
      "Epoch 16, loss: 20588.748144\n",
      "Epoch 17, loss: 20582.500915\n",
      "Epoch 18, loss: 20576.223963\n",
      "Epoch 19, loss: 20569.925223\n",
      "Epoch 20, loss: 20563.931992\n",
      "Epoch 21, loss: 20558.162311\n",
      "Epoch 22, loss: 20551.929203\n",
      "Epoch 23, loss: 20546.289948\n",
      "Epoch 24, loss: 20540.327955\n",
      "Epoch 25, loss: 20534.713818\n",
      "Epoch 26, loss: 20529.088638\n",
      "Epoch 27, loss: 20523.625464\n",
      "Epoch 28, loss: 20518.094450\n",
      "Epoch 29, loss: 20512.461195\n",
      "Epoch 30, loss: 20507.244844\n",
      "Epoch 31, loss: 20502.052554\n",
      "Epoch 32, loss: 20496.575501\n",
      "Epoch 33, loss: 20491.410439\n",
      "Epoch 34, loss: 20486.560033\n",
      "Epoch 35, loss: 20481.376374\n",
      "Epoch 36, loss: 20476.566743\n",
      "Epoch 37, loss: 20471.451445\n",
      "Epoch 38, loss: 20466.695058\n",
      "Epoch 39, loss: 20461.939196\n",
      "Epoch 40, loss: 20457.208653\n",
      "Epoch 41, loss: 20452.634393\n",
      "Epoch 42, loss: 20447.993697\n",
      "Epoch 43, loss: 20443.217760\n",
      "Epoch 44, loss: 20438.853763\n",
      "Epoch 45, loss: 20434.317030\n",
      "Epoch 46, loss: 20430.106074\n",
      "Epoch 47, loss: 20425.752083\n",
      "Epoch 48, loss: 20421.382080\n",
      "Epoch 49, loss: 20417.067434\n",
      "Epoch 50, loss: 20412.900963\n",
      "Epoch 51, loss: 20408.742043\n",
      "Epoch 52, loss: 20404.552933\n",
      "Epoch 53, loss: 20400.471441\n",
      "Epoch 54, loss: 20396.544559\n",
      "Epoch 55, loss: 20392.532369\n",
      "Epoch 56, loss: 20388.813127\n",
      "Epoch 57, loss: 20384.722974\n",
      "Epoch 58, loss: 20380.873013\n",
      "Epoch 59, loss: 20377.273847\n",
      "Epoch 60, loss: 20373.258692\n",
      "Epoch 61, loss: 20369.668565\n",
      "Epoch 62, loss: 20366.000116\n",
      "Epoch 63, loss: 20362.510248\n",
      "Epoch 64, loss: 20358.905873\n",
      "Epoch 65, loss: 20355.262861\n",
      "Epoch 66, loss: 20351.709345\n",
      "Epoch 67, loss: 20348.480659\n",
      "Epoch 68, loss: 20344.933307\n",
      "Epoch 69, loss: 20341.324732\n",
      "Epoch 70, loss: 20338.138994\n",
      "Epoch 71, loss: 20334.949973\n",
      "Epoch 72, loss: 20331.501044\n",
      "Epoch 73, loss: 20328.206649\n",
      "Epoch 74, loss: 20325.148721\n",
      "Epoch 75, loss: 20321.809245\n",
      "Epoch 76, loss: 20318.829739\n",
      "Epoch 77, loss: 20315.766011\n",
      "Epoch 78, loss: 20312.532375\n",
      "Epoch 79, loss: 20309.651693\n",
      "Epoch 80, loss: 20306.364795\n",
      "Epoch 81, loss: 20303.584517\n",
      "Epoch 82, loss: 20300.571185\n",
      "Epoch 83, loss: 20297.714120\n",
      "Epoch 84, loss: 20294.931857\n",
      "Epoch 85, loss: 20291.982229\n",
      "Epoch 86, loss: 20289.142612\n",
      "Epoch 87, loss: 20286.198533\n",
      "Epoch 88, loss: 20283.604628\n",
      "Epoch 89, loss: 20280.759473\n",
      "Epoch 90, loss: 20278.040433\n",
      "Epoch 91, loss: 20275.508998\n",
      "Epoch 92, loss: 20272.847397\n",
      "Epoch 93, loss: 20270.280058\n",
      "Epoch 94, loss: 20267.576265\n",
      "Epoch 95, loss: 20265.073381\n",
      "Epoch 96, loss: 20262.503091\n",
      "Epoch 97, loss: 20259.910777\n",
      "Epoch 98, loss: 20257.395900\n",
      "Epoch 99, loss: 20254.967515\n",
      "Accuracy after training for 100 epochs:  0.214\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=0.1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "reg_strengths = [1e-4, 1e-5, 1e-6]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "# TODO use validation set to find the best hyperparameters\n",
    "# hint: for best results, you might need to try more values for learning rate and regularization strength \n",
    "# than provided initially\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('dlcourse')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "f4cc37f31b004d74a7714c1a5dec4ec9f24ad210ca41b99095a749bba2773dbc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
